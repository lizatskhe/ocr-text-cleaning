{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx4Qvd1Wc_VO"
   },
   "source": [
    "# **OCR Text Correction - Yelizaveta Tskhe**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLELUO_bXVgR"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-MZlosrTP8G",
    "outputId": "fb3b0c64-51f5-493f-f9bb-8788a1155e12"
   },
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MiYAFDiTHv1",
    "outputId": "cf4bb771-1a10-4f0a-bd1c-8e287bfe11b7"
   },
   "outputs": [],
   "source": [
    "pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tq8aBROuLwcG",
    "outputId": "076c9ab7-dc97-442e-a94a-1ed41c31c8c1"
   },
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyfsvpFwUIcN",
    "outputId": "42391a45-e095-44a3-8b16-a41316f1478c"
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e0Bof8ycYEty",
    "outputId": "4f166ed1-25fb-4b61-d276-0e58e2f8467e"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPgWdGboXVOP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tabulate import tabulate\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_S4VRQ4YTeIb",
    "outputId": "f4269e44-6fec-4d86-d6eb-eda686978e1f"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrZPTRyuXd9z",
    "outputId": "142e4cb2-bfb1-406b-ddb9-e8b8fe957f66"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17285,
     "status": "ok",
     "timestamp": 1749234795507,
     "user": {
      "displayName": "YELIZAVETA TSKHE",
      "userId": "13998830846908277279"
     },
     "user_tz": -120
    },
    "id": "lVXjb9xC_HHT",
    "outputId": "d2b172a7-258c-4063-8d49-b7b85e5668ca"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOV5nsBlX9ma"
   },
   "source": [
    "# Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1JwjonpOgfc"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0382570a41b44a639c6dca9d781544d5",
      "200e2d7985e4478d81be2bd8c4421b88",
      "f4e4878c8f5b4ec0ae0494738b784277",
      "6e5c65c36f0e457f812edc064157d4c2",
      "ca31d06d7f6740aa9857b3b254d3203b",
      "26fc5eddb4174cb0a828013cee2b6403",
      "34e480490c954d06bc832a397225d69e",
      "94c5398ef3974ee2a56b10c7aa2a9a04",
      "a2a9ff806bed4cbaa0c8312b68d33f22",
      "1f065505a2614ff1b3d4c6d92f34c91f",
      "b76fe11766e0448683191c3a4de95c3d",
      "78b8ac41567444368ee3ccc551228360",
      "96644b22dc104a52a3dd522ea5ee0bf1",
      "b60e75399a6449bba952823799c7f869",
      "1841a49439a64b0daf62bb11ab11a668",
      "f8288f1fa1fb46ff8a072228eda9ca20",
      "3011280b7a334220890492c013081ee2",
      "7c9f7176587c4348a3b0a1229d163c1b",
      "7e6abc25b0a347bbba83f9b68217c578",
      "e0f45a5e211d482f947dfe707d0ee926",
      "9a7be3995fcf41009548611052b9aee8",
      "1d999f3bde914467918668fcd9a2af9f",
      "6605528c38384f46bb2a9590907d3dc3",
      "81da3675d0be499a926ef7df8b31dc9c",
      "ce48c2fb0dd443b58812e4fde58938aa",
      "8affd6f9980649eca70a8dfea46d6ff5",
      "da47800c1c454db2904c3a4ad9023a08",
      "60b513b968ff4f369501ef129faa25fd",
      "93a523858ac741a1a78fe3573a030757",
      "2661e5b2dcaf4be78db0896bfe0521c8",
      "ac7795583bd94b72b44d8368128063ee",
      "79a83bdb79af423594594c7c92306ffb",
      "eafd107ff49a4f51b1de746c33834ca9",
      "f9364c18c8504c4895f00b93a0a87d97",
      "1e73d245b7cd44a68896ca78f24b49b8",
      "cc54fe5331db422a9ab96b1f617d4b35",
      "cc149232801f4ac4b8b96c92080b1c43",
      "ac585b647799421499a33a4615336b32",
      "b7fdb45b2e0f489d919e300832a0b9c2",
      "84127e732385483b92a18f6a915802d8",
      "52d6ccdbcfd443318868d1764e6da708",
      "8863d496f5574c84a83af0fc28f00e5d",
      "49d4d793ca4c4a8eb80007984a3b57a5",
      "7fcb00e18905444487b462d93c54dcab",
      "0d2e089aaa604e8c975950e8343554d4",
      "d0d9c6e93a234c53926bf9b6c9c14424",
      "3c3c2003db9046a2bec51fb32ef4b614",
      "b73337804abc4af38c84267a4e9add71",
      "1c9c321e3e54453a8d0ebec1ea4b1177",
      "8e51c7640dff48929f75eae339fcdbaa",
      "871c7ddbdbeb4e9ab38140a519e6da04",
      "2d15fe09599e4947bfff0b9b1f5f649d",
      "6b39c94c0c2e4ee49ec17134d8d6fbed",
      "56b60603079c442e8170cdd8b0fe2d8a",
      "810c5a8b6db94310a4bb87e4ddc73490",
      "3c2c20233b5f46bfa425d724e8d926fa",
      "6aa6a21ced4d46768ddc156b51aa5b23",
      "eb73576e081448fea084e1e6fefd28ec",
      "25d2cb2a31f34d3f93354cb8009f5f59",
      "847cc01ec3344a9e9904d9e20f476c3d",
      "dc2603ca762c451bb0ab3436a3c898b6",
      "c76a7ab45214428e9d06673b3699d3fe",
      "50e2ffd88d074f5e90312d2a70804f65",
      "a951bed430434d2890de0b628d1e9658",
      "f264d278db66480eb4d3abb79f6e56bc",
      "2bb14f1fe46e4526944ab4fbf53bd532",
      "c007094e4d384c12bfb59bf6d68fe92f",
      "f138df105d6c4f16a222b7707c042a59",
      "f88edf61e9974ce69d481961093045a1",
      "0029621a0ef3485193b9df0b3f9a1585",
      "3b3371588e0543d780d3267922c39912",
      "b29f542d5840493a8bfc7143fabe152a",
      "5d1de052842b4e82b6e5da639d019b9e",
      "6372dc0dcba841f4a08894592044dfb1",
      "12eb5090754547cf8e4cd666d0de3751",
      "7142047a09c64193b415ca990535b3f8",
      "390f0cb544314e729ac249cc186c67d6",
      "5072fac88dea4c7881e3575871c7b5cc",
      "c6f6d91d7a16458c9abee87c68ba5e74",
      "9e03507db3574db3a5a04b1d38e50f99",
      "438884c75e7e41688ccab274e9600a7e",
      "47ca3382ad9849bda12a267b99ea33be",
      "f43bebe27d954affa1723600c60bc7f6",
      "01517fadc994423c98fc596c7fd3d234",
      "b6b21f487a4f47e0be5e96ef76d12a0e",
      "f60dc09560ca447b9c0595e7e9e04790",
      "1db3ac3f2e114992976215062bd783a3",
      "367d0cf4e40f45d9bca787565c60da6c",
      "1419b5709c504016b8dea00ef5d2453f",
      "a3d8d0d77c9a4f9383b4d0efea4d7b6f",
      "33847a1d40d54d5388ca840053af3885",
      "a49e2009554f49639faa470bef8758cb",
      "e037f48ecc564a4ea08abdf01fa0b0ee",
      "df87c980fc6844bfb35584baead99e12",
      "9b23e8684058414ca3dfcbb827c3b8e7",
      "a926d3b6d0954c28920f761b8ab12fe7",
      "c4ef349c5f31481d8c6584de4be1d137",
      "d1fad825dbcb4a56a41ea3bc6af1711a",
      "eb5462e000e147aea76e495181fbb4b9",
      "699f26248d2c40f187da2580a4c00e88",
      "eb8f26b3f113436888871a1f73675297",
      "840f80f9a0ec42e1b031378c3f8f0458",
      "d1c62a2d639044fb9fde0bdc0c35e2ec",
      "414194b824564462ade465c1ae1766ca",
      "bda58f41294a434ba0b602af6e2250ea",
      "7a4df34db6f74a0685482e4bd57e4613",
      "74beda355d00421ebc60ca07c2bef8cf",
      "87f35ee3eaa942fcaf9b690a71c8e82d",
      "417f0d144c5941c4ac00e3543c7ac77b",
      "63c6d649feb6482ba7dc1fc7416e71b6",
      "5fcd2d3af6c043fbaaed77dde9dd142a",
      "4d84042cb87a4a2e85636b562cfc5d6e",
      "7da741872e774e2ca0920b0cf348d458",
      "8006ced96b2b432ca3511737bb51d6f4",
      "33b1db1a5362485bbfec12d16adf7780",
      "1850d2570c1a4c8a8aa13db1d8bacf2e",
      "f8d43b2b82b8442dbf00f40c4d994ed8",
      "9312a84815ca4e1e8acbd6a248c11484",
      "c8fa106cf4fe42218d5360eb2f87b54d",
      "286d7d1167c94d82bb72f251e7f7e676",
      "0629f0b06fe54f91b48f6b61d857d264",
      "d32555d877db41a7912cf55f4c2449ca",
      "45551d3b83724b56a977259d711fd68c",
      "77a48d98f68a445c96c32cf1e4147d06",
      "cf7a8c3debf84b2ea6a2716d53ca5ae9",
      "4af145a7bcf341d1bbcd50ac64439372",
      "eaeef9d5cb094252af203033ff09f6fc",
      "39f9d90a4d834a4e8dce12f9c8c30ea6",
      "7d56fc4e2a264aabb8e79d941a8c45a6",
      "a0d22e9587d041febceeea50e867cf56",
      "3be067b407314bb59de5c42479c87970",
      "40ed514131584dbb8494b3bb24965729"
     ]
    },
    "id": "qOO7TRPbL3BP",
    "outputId": "78dc6035-86b8-43a0-852c-46d9ea92befd"
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True  # <- allows model to use its custom chat template\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with open(\"the_vampyre_ocr_5k.json\", \"r\") as f:\n",
    "    ocr_data = json.  load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for key, ocr_text in list(ocr_data.items())[:1]:\n",
    "\n",
    "    sentences = [s.strip() + \".\" for s in ocr_text.split('.') if s.strip()]\n",
    "    chunk_size = 6\n",
    "    chunks = [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "    corrected_chunks = []\n",
    "\n",
    "    for i, chunk_sentences in enumerate(tqdm(chunks, desc=\"correcting chunks\")):\n",
    "\n",
    "        chunk_text = \" \".join(chunk_sentences)\n",
    "\n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\":\n",
    "                    \"You are an expert text editor. The following text was generated by OCR (Optical Character Recognition) from an old book scan and contains typical OCR errors. Your job is to correct the text without altering its meaning or style. Return ONLY the corrected text, and do NOT add any introductions, summaries, or comments. \"\n",
    "\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"TH1S 1S A SAMP1E TEXT FR0M AN 0LD SCANNED B00K.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"THIS IS A SAMPLE TEXT FROM AN OLD SCANNED BOOK.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"He w3nt to the cast1e at n1ght.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"He went to the castle at night.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"input: {chunk_text}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        chat_prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "\n",
    "\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        output_ids = outputs[0]\n",
    "\n",
    "        generated_ids = output_ids[input_len:]\n",
    "        cleaned_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        corrected_chunks.append(cleaned_output)\n",
    "\n",
    "    final_clean_text = \" \".join(corrected_chunks)\n",
    "\n",
    "    results.append({\n",
    "        \"input\": ocr_text,\n",
    "        \"corrected\": final_clean_text\n",
    "    })\n",
    "\n",
    "with open(\"ocr-llama.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"saved corrected text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t4HLEUKk49c"
   },
   "source": [
    "# Phi-3 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535,
     "referenced_widgets": [
      "e2f49f520b584ffeb18cf1d9662023f9",
      "65649984a61a49ff8d427f25f6954816",
      "8829a13e3be644ce981bd84cce11bbac",
      "d32a735dacc24181ad480d810646bd50",
      "c793c31cbd3642ccbae0ac11e9771e99",
      "702abe2b33e349c3b725cba2fd07327a",
      "dcf70c1874df4dd6b672ace1a00eeb58",
      "784ca41996b44ab2951e7c840f650ca6",
      "e7270532eb664cb3b0455bb049f9595f",
      "5626173c97304076b783300b6347ef95",
      "03a8937c29a643b8957d47b5d1378248"
     ]
    },
    "id": "RJ67-qJdkWnN",
    "outputId": "94e9f0b6-02ea-4233-9dfa-53c23ba22a6c"
   },
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with open(\"the_vampyre_ocr_5k.json\", \"r\") as f:\n",
    "    ocr_data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for key, ocr_text in list(ocr_data.items())[:1]:  # adjust slice as needed\n",
    "\n",
    "    sentences = [s.strip() + \".\" for s in ocr_text.split('.') if s.strip()]\n",
    "    chunk_size = 4\n",
    "    chunks = [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "    corrected_chunks = []\n",
    "\n",
    "    for i, chunk_sentences in enumerate(tqdm(chunks, desc=\"correcting chunks\")):\n",
    "        chunk_text = \" \".join(chunk_sentences)\n",
    "\n",
    "        prompt = f\"\"\"Fix OCR errors in this text. Output ONLY the corrected text, nothing else:\n",
    "\n",
    "{chunk_text}\n",
    "\n",
    "Corrected text:\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # appropriate max_new_tokens based on input\n",
    "        input_word_count = len(chunk_text.split())\n",
    "        # allow up to 1.5x the input length\n",
    "        max_tokens = max(input_word_count + 100, int(input_word_count * 1.5))\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,  # dynamic based on input size\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        output_ids = outputs[0]\n",
    "        generated_ids = output_ids[input_len:]\n",
    "        cleaned_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        stop_phrases = [\n",
    "            \"\\n\\n\", \"\\ninput:\", \"\\nFix OCR\", \"\\nOutput\", \"\\nCorrected text:\",\n",
    "            \"\\nExamples:\", \"\\nsummary\", \"Explanation:\", \"Your job\", \"Note:\",\n",
    "            \"You are\", \"TH1S 1S\", \"Here's\", \"This task\", \"**Original\",\n",
    "            \"plaintext\", \"After careful\", \"based on your instructions\",\n",
    "            \"You are tasked\", \"Your objective\", \"Additionally\", \"Ensure\",\n",
    "            \"Lastly\", \"Provide explanatory\"\n",
    "        ]\n",
    "\n",
    "        # stop phrase filtering ,preserve all text before the stop phrase\n",
    "        original_output = cleaned_output\n",
    "        for stop in stop_phrases:\n",
    "            if stop in cleaned_output:\n",
    "                cleaned_output = cleaned_output.split(stop)[0].strip()\n",
    "\n",
    "        # careful line filtering\n",
    "        lines = cleaned_output.split('\\n')\n",
    "        filtered_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # skip obviously problematic lines, keep most content\n",
    "            if (line and\n",
    "                not line.startswith('**') and\n",
    "                not line.startswith('###') and\n",
    "                not line.lower().startswith('here is') and\n",
    "                not line.lower().startswith('this task') and\n",
    "                'step-by-step' not in line.lower() and\n",
    "                'correction process' not in line.lower()):\n",
    "                filtered_lines.append(line)\n",
    "\n",
    "        if filtered_lines:\n",
    "            cleaned_output = '\\n'.join(filtered_lines)\n",
    "\n",
    "        #  if we lost too much content, use less aggressive filtering\n",
    "        original_word_count = len(chunk_text.split())\n",
    "        cleaned_word_count = len(cleaned_output.split())\n",
    "\n",
    "        # if we lost more than 50% of the content, fall back to simpler filtering\n",
    "        if cleaned_word_count < original_word_count * 0.5:\n",
    "            print(f\"filtering removed too much content ({cleaned_word_count}/{original_word_count} words)\")\n",
    "            # just remove obvious hallucination patterns but keep most text\n",
    "            for stop in [\"\\nYou are\", \"\\nThis task\", \"\\nHere's\", \"**Original\"]:\n",
    "                if stop in original_output:\n",
    "                    cleaned_output = original_output.split(stop)[0].strip()\n",
    "                    break\n",
    "            else:\n",
    "                cleaned_output = original_output\n",
    "\n",
    "        corrected_chunks.append(cleaned_output)\n",
    "\n",
    "        if i < 26:\n",
    "            print(f\"chunk {i}: input: {len(chunk_text.split())}, output: {len(cleaned_output.split())}\")\n",
    "\n",
    "    final_clean_text = \" \".join(corrected_chunks)\n",
    "\n",
    "    results.append({\n",
    "        \"input\": ocr_text,\n",
    "        \"corrected\": final_clean_text\n",
    "    })\n",
    "\n",
    "with open(\"ocr-phi3.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"saved corrected text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K__LBTm7qGcu"
   },
   "source": [
    "# LLM-as-a-judge evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZgoPJhIqwoK"
   },
   "source": [
    "### EVALUATION RUBRICS\n",
    "\n",
    "1. **Completely unacceptable** - The output has no resemblance to the original text's intent. It is either unreadable, full of gibberish, or hallucinated content. Key words are missing or invented.\n",
    "\n",
    "2. **Severely flawed** - Major OCR errors remain (e.g., symbols in place of letters, obvious misreadings), and the sentence has significant distortions. The text may be partially readable but is not semantically or syntactically correct.\n",
    "\n",
    "3. **Partially correct** - The core meaning of the text is retained, but several minor OCR errors persist (e.g., digit-letter confusions, spacing issues). The output may contain small artifacts but is mostly understandable.\n",
    "\n",
    "4. **Good correction** - Most OCR errors have been corrected. The sentence is fluent, understandable, and faithful to the original text. Some very minor issues (like style inconsistencies or awkward phrasing) may be present.\n",
    "\n",
    "5. **Perfect correction** - The output is indistinguishable from human proofreading: fluent, coherent, completely free of OCR artifacts, and faithful in both style and content to the intended meaning of the original scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXDAYhCNVIuf"
   },
   "outputs": [],
   "source": [
    "class OCRJudge:\n",
    "    def __init__(self, api_key: str, model_name: str = \"gemini-2.0-flash\"):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    def create_prompt(self, ocr_input: str, model_a_output: str, model_b_output: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are an impartial judge. Evaluate the model's output for correcting OCR text.\n",
    "\n",
    "Original OCR input:\n",
    "\"{ocr_input}\"\n",
    "\n",
    "Model A output:\n",
    "\"{model_a_output}\"\n",
    "\n",
    "Model B output:\n",
    "\"{model_b_output}\"\n",
    "\n",
    "Evaluate output on a scale of 1–5 for each of the following criteria:\n",
    "1. Completely unacceptable - The output has no resemblance to the original text's intent. It is either unreadable, full of gibberish, or hallucinated content. Key words are missing or invented.\n",
    "2. Severely flawed - Major OCR errors remain (e.g., symbols in place of letters, obvious misreadings), and the sentence has significant distortions. The text may be partially readable but is not semantically or syntactically correct.\n",
    "3. Partially correct - The core meaning of the text is retained, but several minor OCR errors persist (e.g., digit-letter confusions, spacing issues). The output may contain small artifacts but is mostly understandable.\n",
    "4. Good correction - Most OCR errors have been corrected. The sentence is fluent, understandable, and faithful to the original text. Some very minor issues (like style inconsistencies or awkward phrasing) may be present.\n",
    "5. Perfect correction - The output is indistinguishable from human proofreading: fluent, coherent, completely free of OCR artifacts, and faithful in both style and content to the intended meaning of the original scan.\n",
    "\n",
    "Return only:\n",
    "Model A score: X\n",
    "Model B score: Y\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def parse_scores(self, response: str) -> Tuple[Optional[int], Optional[int]]:\n",
    "        try:\n",
    "            model_a_match = re.search(r'model A score:\\s*(\\d+)', response, re.IGNORECASE)\n",
    "            model_b_match = re.search(r'model B score:\\s*(\\d+)', response, re.IGNORECASE)\n",
    "\n",
    "            model_a_score = int(model_a_match.group(1)) if model_a_match else None\n",
    "            model_b_score = int(model_b_match.group(1)) if model_b_match else None\n",
    "\n",
    "            if model_a_score and (model_a_score < 1 or model_a_score > 5):\n",
    "                model_a_score = None\n",
    "            if model_b_score and (model_b_score < 1 or model_b_score > 5):\n",
    "                model_b_score = None\n",
    "\n",
    "            return model_a_score, model_b_score\n",
    "        except Exception as e:\n",
    "            print(f\"error : {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def evaluate_single_item(self, item: Dict, model_a_key: str = \"llama_corrected\",\n",
    "                           model_b_key: str = \"phi3_corrected\") -> Tuple[Optional[int], Optional[int]]:\n",
    "        try:\n",
    "            ocr_input = item[\"input\"]\n",
    "            model_a_output = item[model_a_key]\n",
    "            model_b_output = item[model_b_key]\n",
    "\n",
    "            prompt = self.create_prompt(ocr_input, model_a_output, model_b_output)\n",
    "\n",
    "            response = self.model.generate_content(prompt)\n",
    "\n",
    "            model_a_score, model_b_score = self.parse_scores(response.text)\n",
    "\n",
    "            print(f\"evaluated: A={model_a_score}, B={model_b_score}\")\n",
    "\n",
    "            return model_a_score, model_b_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def evaluate_dataset(self, data: List[Dict], model_a_key: str = \"llama_corrected\",\n",
    "                        model_b_key: str = \"phi3_corrected\",\n",
    "                        llm_score_a_key: str = \"llm_score_llama\",\n",
    "                        llm_score_b_key: str = \"llm_score_phi3\",\n",
    "                        delay: float = 1.0) -> List[Dict]:\n",
    "        updated_data = data.copy()\n",
    "\n",
    "        for i, item in enumerate(updated_data):\n",
    "            print(f\"processing {i+1}/{len(updated_data)}\")\n",
    "\n",
    "            if (item.get(llm_score_a_key) is not None and\n",
    "                item.get(llm_score_b_key) is not None):\n",
    "                print(f\"item {i+1} already evaluated\")\n",
    "                continue\n",
    "\n",
    "            model_a_score, model_b_score = self.evaluate_single_item(\n",
    "                item, model_a_key, model_b_key\n",
    "            )\n",
    "\n",
    "            item[llm_score_a_key] = model_a_score\n",
    "            item[llm_score_b_key] = model_b_score\n",
    "\n",
    "            # delay to avoid rate limiting\n",
    "            if delay > 0 and i < len(updated_data) - 1:\n",
    "                time.sleep(delay)\n",
    "\n",
    "        return updated_data\n",
    "\n",
    "def main():\n",
    "    API_KEY = \"AIzaSyDpzOanDrmS9FY3DcSN63zYEPoVx9tU_b4\"\n",
    "    INPUT_FILE = \"aligned_sentences.json\"\n",
    "\n",
    "    OUTPUT_FILE = \"ocr-judge.json\"\n",
    "\n",
    "    judge = OCRJudge(API_KEY)\n",
    "\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"loaded {len(data)} items from {INPUT_FILE}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"error: file not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"error json: {e}\")\n",
    "        return\n",
    "\n",
    "    batch = data[:5]\n",
    "    # batch = data[5:10]\n",
    "    # batch = data[10:15]\n",
    "    # batch = data[15:20]\n",
    "    # batch = data[20:25]\n",
    "\n",
    "    print(\"evaluation started\")\n",
    "    evaluated_data = judge.evaluate_dataset(\n",
    "        # data,\n",
    "        batch,\n",
    "        model_a_key=\"llama_corrected\",\n",
    "        model_b_key=\"phi3_corrected\",\n",
    "        delay=1.0  # 1 second delay between requests\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(evaluated_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"results saved to {OUTPUT_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error : {e}\")\n",
    "\n",
    "    llama_scores = [item.get(\"llm_score_llama\") for item in evaluated_data if item.get(\"llm_score_llama\") is not None]\n",
    "    phi3_scores = [item.get(\"llm_score_phi3\") for item in evaluated_data if item.get(\"llm_score_phi3\") is not None]\n",
    "\n",
    "    if llama_scores:\n",
    "        print(f\"\\nllama scores - count: {len(llama_scores)}, average: {sum(llama_scores)/len(llama_scores):.2f}\")\n",
    "    if phi3_scores:\n",
    "        print(f\"phi3 scores - count: {len(phi3_scores)}, average: {sum(phi3_scores)/len(phi3_scores):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q9XnR6KoXVp"
   },
   "source": [
    "# Human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m9Ej5j4oXVq"
   },
   "source": [
    "**Sentence 1:**\n",
    "- **Llama: 3** - Incorrectly changed \"Hadagni\" to \"hangman\" and \"florid\" to \"fluid\", altering meaning\n",
    "- **Phi3: 2** - Sentence is truncated and contains \"entranely\" error\n",
    "\n",
    "**Sentence 2:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction (extra space is negligible)\n",
    "\n",
    "**Sentence 3:**\n",
    "- **Llama: 5** - All OCR errors correctly fixed\n",
    "- **Phi3: 4** - Good but missing \"heart and\" (only says \"through the body\")\n",
    "\n",
    "**Sentence 4:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction\n",
    "\n",
    "**Sentence 5:**\n",
    "- **Llama: 4** - Good correction but changed \"agents upon\" to \"infected and attack\"\n",
    "- **Phi3: 2** - \"clinging up to\" is nonsensical\n",
    "\n",
    "**Sentence 6:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction\n",
    "\n",
    "**Sentence 7:**\n",
    "- **Llama: 3** - \"Chief Baily\" is incorrect spelling\n",
    "- **Phi3: 1** - No correction attempted\n",
    "\n",
    "**Sentence 8:**\n",
    "- **Llama: 4** - Good but changed \"rodomontade\" to \"fable\"\n",
    "- **Phi3: 2** - \"irres0r3tance\" contains OCR errors\n",
    "\n",
    "**Sentence 9:**\n",
    "- **Llama: 5** - Excellent corrections throughout\n",
    "- **Phi3: 3** - \"oppressed by his infernal vampiric visions\" doesn't make sense\n",
    "\n",
    "**Sentence 10:**\n",
    "- **Llama: 4** - Good but some word substitutions (\"Therefrom\", \"crossing\")\n",
    "- **Phi3: 2** - Multiple errors (\"suck thy blood\", \"shall slip\")\n",
    "\n",
    "**Sentence 11:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction\n",
    "\n",
    "**Sentence 12:**\n",
    "- **Llama: 4** - Good corrections but changed \"thine\" to \"her\" in one place\n",
    "- **Phi3: 2** - Multiple OCR errors remain (\"inark\", \"hor\", \"whither sborn\")\n",
    "\n",
    "**Sentence 13:**\n",
    "- **Llama: 4** - Good with minor word changes\n",
    "- **Phi3: 3** - Several errors remain (\"baggard\", \"thefe\", \"spectacle\")\n",
    "\n",
    "**Sentence 14:**\n",
    "- **Llama: 4** - Good but some wording changes\n",
    "- **Phi3: 3** - Significant alterations that change meaning\n",
    "\n",
    "**Sentence 15:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction\n",
    "\n",
    "**Sentence 16:**\n",
    "- **Llama: 4** - Good but changed \"veracious\" to \"venerable\"\n",
    "- **Phi3: 2** - Missing \"Tournefort\" and has \"eyewit0rst\" OCR error\n",
    "\n",
    "**Sentence 17:**\n",
    "- **Llama: 5** - Excellent corrections\n",
    "- **Phi3: 3** - Keeps OCR errors in proper nouns\n",
    "\n",
    "**Sentence 18:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 1** - Major hallucination with completely different content\n",
    "\n",
    "**Sentence 19:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction\n",
    "\n",
    "**Sentence 20:**\n",
    "- **Llama: 5** - Perfect correction  \n",
    "- **Phi3: 5** - Perfect correction\n",
    "\n",
    "**Sentence 21:**\n",
    "- **Llama: 4** - Good but added unnecessary \"like that\"\n",
    "- **Phi3: 2** - \"oak\" instead of \"cheek\" is a major error\n",
    "\n",
    "**Sentence 22:**\n",
    "- **Llama: 5** - Perfect (changing \"house\" to \"ball\" is acceptable)\n",
    "- **Phi3: 3** - \"weight of nothing\" and \"possession\" are incorrect\n",
    "\n",
    "**Sentence 23:**\n",
    "- **Llama: 3** - Several incorrect word choices\n",
    "- **Phi3: 2** - Missing portions and incorrect words\n",
    "\n",
    "**Sentence 24:**\n",
    "- **Llama: 2** - \"semi-otics\" is nonsensical\n",
    "- **Phi3: 3** - Changes meaning but more coherent\n",
    "\n",
    "**Sentence 25:**\n",
    "- **Llama: 5** - Perfect correction\n",
    "- **Phi3: 5** - Perfect correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2Tg7F5noXVr",
    "outputId": "91862f57-d3ed-426b-f8f9-4baa78aab119"
   },
   "outputs": [],
   "source": [
    "with open(\"ocr-judge.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "human_scores_llama = [3, 5, 5, 5, 4, 5, 3, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 3, 2, 5]\n",
    "human_scores_phi3 = [2, 5, 4, 5, 2, 5, 1, 2, 3, 2, 5, 2, 3, 3, 5, 2, 3, 1, 5, 5, 2, 3, 2, 3, 5]\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    item[\"human_score_llama\"] = human_scores_llama[i]\n",
    "    item[\"human_score_phi3\"] = human_scores_phi3[i]\n",
    "\n",
    "with open(\"aligned_sentences_with_human_scores.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"human scores saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc3kn-36289T"
   },
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkmYXhEMQvsE",
    "outputId": "4360aeb1-e44a-42c8-f701-6fb061d3b025"
   },
   "outputs": [],
   "source": [
    "#### ROUGE SCORES\n",
    "\n",
    "with open(\"aligned_sentences_with_human_scores.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "for item in data:\n",
    "    reference = item[\"reference\"]\n",
    "\n",
    "    llama_scores = scorer.score(reference, item[\"llama_corrected\"])\n",
    "    item[\"llama_rouge-1\"] = llama_scores[\"rouge1\"].fmeasure\n",
    "    item[\"llama_rouge-2\"] = llama_scores[\"rouge2\"].fmeasure\n",
    "    item[\"llama_rouge-L\"] = llama_scores[\"rougeL\"].fmeasure\n",
    "\n",
    "    phi3_scores = scorer.score(reference, item[\"phi3_corrected\"])\n",
    "    item[\"phi3_rouge-1\"] = phi3_scores[\"rouge1\"].fmeasure\n",
    "    item[\"phi3_rouge-2\"] = phi3_scores[\"rouge2\"].fmeasure\n",
    "    item[\"phi3_rouge-L\"] = phi3_scores[\"rougeL\"].fmeasure\n",
    "\n",
    "with open(\"aligned_sentences_with_rouge.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"rouge scores saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctqw9Q9bRUtk",
    "outputId": "12a3c7b9-9cc1-48bb-a04c-73827f5c679a"
   },
   "outputs": [],
   "source": [
    "### CORRELATIONS\n",
    "\n",
    "with open(\"aligned_sentences_with_rouge.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def extract_scores(prefix):\n",
    "    return {\n",
    "        \"human\": [item[f\"human_score_{prefix}\"] for item in data if item.get(f\"human_score_{prefix}\") is not None],\n",
    "        \"llm\": [item[f\"llm_score_{prefix}\"] for item in data if item.get(f\"human_score_{prefix}\") is not None],\n",
    "        \"rouge1\": [item[f\"{prefix}_rouge-1\"] for item in data if item.get(f\"human_score_{prefix}\") is not None],\n",
    "        \"rouge2\": [item[f\"{prefix}_rouge-2\"] for item in data if item.get(f\"human_score_{prefix}\") is not None],\n",
    "        \"rougeL\": [item[f\"{prefix}_rouge-L\"] for item in data if item.get(f\"human_score_{prefix}\") is not None],\n",
    "    }\n",
    "\n",
    "llama = extract_scores(\"llama\")\n",
    "phi3 = extract_scores(\"phi3\")\n",
    "\n",
    "def compute_corr(human, metric):\n",
    "    if len(set(human)) < 2 or len(set(metric)) < 2:\n",
    "        return {\"pearson\": None, \"spearman\": None}\n",
    "    return {\n",
    "        \"pearson\": round(pearsonr(human, metric)[0], 4),\n",
    "        \"spearman\": round(spearmanr(human, metric).correlation, 4)\n",
    "    }\n",
    "\n",
    "results = {\n",
    "    \"llama\": {\n",
    "        \"human_vs_llm\": compute_corr(llama[\"human\"], llama[\"llm\"]),\n",
    "        \"human_vs_rouge1\": compute_corr(llama[\"human\"], llama[\"rouge1\"]),\n",
    "        \"human_vs_rouge2\": compute_corr(llama[\"human\"], llama[\"rouge2\"]),\n",
    "        \"human_vs_rougeL\": compute_corr(llama[\"human\"], llama[\"rougeL\"]),\n",
    "    },\n",
    "    \"phi3\": {\n",
    "        \"human_vs_llm\": compute_corr(phi3[\"human\"], phi3[\"llm\"]),\n",
    "        \"human_vs_rouge1\": compute_corr(phi3[\"human\"], phi3[\"rouge1\"]),\n",
    "        \"human_vs_rouge2\": compute_corr(phi3[\"human\"], phi3[\"rouge2\"]),\n",
    "        \"human_vs_rougeL\": compute_corr(phi3[\"human\"], phi3[\"rougeL\"]),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"correlation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "\n",
    "print(\"correlation results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSAGCdEJSy8k",
    "outputId": "28ca44f3-0a79-4d64-f13b-0364c1332934"
   },
   "outputs": [],
   "source": [
    "with open(\"correlation_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "table_data = []\n",
    "for model in [\"llama\", \"phi3\"]:\n",
    "    for metric in [\"human_vs_llm\", \"human_vs_rouge1\", \"human_vs_rouge2\", \"human_vs_rougeL\"]:\n",
    "        row = [\n",
    "            model.upper(),\n",
    "            metric.replace(\"human_vs_\", \"\").upper(),\n",
    "            results[model][metric][\"pearson\"] if results[model][metric][\"pearson\"] is not None else \"N/A\",\n",
    "            results[model][metric][\"spearman\"] if results[model][metric][\"spearman\"] is not None else \"N/A\"\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "\n",
    "headers = [\"Model\", \"Metric\", \"Pearson\", \"Spearman\"]\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxLOIk8mwrGF"
   },
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_A_gn8BwqH2",
    "outputId": "d5fb937e-9c66-4c9a-d2ff-6ea9893b6925"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "with open(\"aligned_sentences_with_human_scores.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "llm_scores_llama = [item[\"llm_score_llama\"] for item in data]\n",
    "llm_scores_phi3 = [item[\"llm_score_phi3\"] for item in data]\n",
    "human_scores_llama = [item[\"human_score_llama\"] for item in data]\n",
    "human_scores_phi3 = [item[\"human_score_phi3\"] for item in data]\n",
    "\n",
    "pearson_llama = pearsonr(llm_scores_llama, human_scores_llama)\n",
    "spearman_llama = spearmanr(llm_scores_llama, human_scores_llama)\n",
    "pearson_phi3 = pearsonr(llm_scores_phi3, human_scores_phi3)\n",
    "spearman_phi3 = spearmanr(llm_scores_phi3, human_scores_phi3)\n",
    "\n",
    "# 1. Correlation Scatter Plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.scatter(human_scores_llama, llm_scores_llama, alpha=0.6, s=100, color='#FF6B6B')\n",
    "ax1.plot([1, 5], [1, 5], 'k--', alpha=0.3)\n",
    "ax1.set_xlabel('Human Scores', fontsize=12)\n",
    "ax1.set_ylabel('LLM Scores', fontsize=12)\n",
    "ax1.set_title('Llama: Human vs LLM Evaluation', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0.5, 5.5)\n",
    "ax1.set_ylim(0.5, 5.5)\n",
    "ax1.text(0.05, 0.95, f'Pearson r = {pearson_llama[0]:.3f}\\nSpearman ρ = {spearman_llama[0]:.3f}',\n",
    "         transform=ax1.transAxes, fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "ax2.scatter(human_scores_phi3, llm_scores_phi3, alpha=0.6, s=100, color='#4ECDC4')\n",
    "ax2.plot([1, 5], [1, 5], 'k--', alpha=0.3)\n",
    "ax2.set_xlabel('Human Scores', fontsize=12)\n",
    "ax2.set_ylabel('LLM Scores', fontsize=12)\n",
    "ax2.set_title('Phi-3: Human vs LLM Evaluation', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0.5, 5.5)\n",
    "ax2.set_ylim(0.5, 5.5)\n",
    "ax2.text(0.05, 0.95, f'Pearson r = {pearson_phi3[0]:.3f}\\nSpearman ρ = {spearman_phi3[0]:.3f}',\n",
    "         transform=ax2.transAxes, fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_scatter_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Correlation Coefficients Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.array([0, 1, 3, 4])\n",
    "width = 0.8\n",
    "colors = ['#FFB6B9', '#FF6F91', '#A3D2CA', '#5EAAA8']\n",
    "\n",
    "correlations = [pearson_llama[0], spearman_llama[0], pearson_phi3[0], spearman_phi3[0]]\n",
    "labels = ['Llama\\nPearson', 'Llama\\nSpearman', 'Phi-3\\nPearson', 'Phi-3\\nSpearman']\n",
    "\n",
    "bars = ax.bar(x, correlations, width, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for bar, corr in zip(bars, correlations):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{corr:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_title('Human-LLM Scores Correlation', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_coefficients_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Score Distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].hist(human_scores_llama, bins=5, range=(0.5, 5.5), alpha=0.7, color='#FFB6B9', edgecolor='black')\n",
    "axes[0, 0].set_title('Human Scores - Llama', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[0, 1].hist(human_scores_phi3, bins=5, range=(0.5, 5.5), alpha=0.7, color='#A3D2CA', edgecolor='black')\n",
    "axes[0, 1].set_title('Human Scores - Phi-3', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 0].hist(llm_scores_llama, bins=5, range=(0.5, 5.5), alpha=0.7, color='#FF6F91', edgecolor='black')\n",
    "axes[1, 0].set_title('LLM Scores - Llama', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 1].hist(llm_scores_phi3, bins=5, range=(0.5, 5.5), alpha=0.7, color='#5EAAA8', edgecolor='black')\n",
    "axes[1, 1].set_title('LLM Scores - Phi-3', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Score Distribution', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('score_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Model Performance Comparison\n",
    "avg_human_llama = np.mean(human_scores_llama)\n",
    "avg_human_phi3 = np.mean(human_scores_phi3)\n",
    "avg_llm_llama = np.mean(llm_scores_llama)\n",
    "avg_llm_phi3 = np.mean(llm_scores_phi3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, [avg_human_llama, avg_human_phi3], width, label='Human Scores', color='#FF6F91')\n",
    "bars2 = ax.bar(x + width/2, [avg_llm_llama, avg_llm_phi3], width, label='LLM Scores', color='#5EAAA8')\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Average Score', fontsize=12)\n",
    "ax.set_title('Average Scores by Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Llama', 'Phi-3'], fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 5.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Results Table (Human-LLM)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [\n",
    "    ['Metric', 'Llama', 'Phi-3'],\n",
    "    ['Average Human Score', f'{avg_human_llama:.2f}', f'{avg_human_phi3:.2f}'],\n",
    "    ['Average LLM Score', f'{avg_llm_llama:.2f}', f'{avg_llm_phi3:.2f}'],\n",
    "    ['Pearson Correlation Coefficient', f'{pearson_llama[0]:.3f}', f'{pearson_phi3[0]:.3f}'],\n",
    "    ['Spearman Correlation Coefficient', f'{spearman_llama[0]:.3f}', f'{spearman_phi3[0]:.3f}'],\n",
    "    # ['P-value (Pearson)', f'{pearson_llama[1]:.4f}', f'{pearson_phi3[1]:.4f}'],\n",
    "    # ['P-value (Spearman)', f'{spearman_llama[1]:.4f}', f'{spearman_phi3[1]:.4f}']\n",
    "]\n",
    "\n",
    "table = ax.table(cellText=table_data, loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.2, 1.8)\n",
    "\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#4ECDC4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, 5):\n",
    "    table[(i, 0)].set_facecolor('#E0E0E0')\n",
    "    table[(i, 0)].set_text_props(weight='bold')\n",
    "\n",
    "# ax.set_title('Performance Metrics', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.savefig('performance_metrics_table.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Confusion heatmap\n",
    "agreement_matrix_llama = np.zeros((5, 5))\n",
    "agreement_matrix_phi3 = np.zeros((5, 5))\n",
    "\n",
    "for h, l in zip(human_scores_llama, llm_scores_llama):\n",
    "    agreement_matrix_llama[5-int(l), int(h)-1] += 1\n",
    "\n",
    "for h, l in zip(human_scores_phi3, llm_scores_phi3):\n",
    "    agreement_matrix_phi3[5-int(l), int(h)-1] += 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(agreement_matrix_llama, annot=True, fmt='g', cmap='YlOrRd',\n",
    "            xticklabels=['1', '2', '3', '4', '5'],\n",
    "            yticklabels=['5', '4', '3', '2', '1'],\n",
    "            cbar_kws={'label': 'Count'}, ax=ax1)\n",
    "ax1.set_xlabel('Human Score', fontsize=12)\n",
    "ax1.set_ylabel('LLM Score', fontsize=12)\n",
    "ax1.set_title('Llama: Human-LLM Score Agreement', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(agreement_matrix_phi3, annot=True, fmt='g', cmap='YlGnBu',\n",
    "            xticklabels=['1', '2', '3', '4', '5'],\n",
    "            yticklabels=['5', '4', '3', '2', '1'],\n",
    "            cbar_kws={'label': 'Count'}, ax=ax2)\n",
    "ax2.set_xlabel('Human Score', fontsize=12)\n",
    "ax2.set_ylabel('LLM Score', fontsize=12)\n",
    "ax2.set_title('Phi-3: Human-LLM Score Agreement', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('score_agreement_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nsaved all figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0SPUg9e70n0",
    "outputId": "706295a9-e5d1-4c22-8b99-6f003b8d01c4"
   },
   "outputs": [],
   "source": [
    "### HUMAN - ROUGE correlation\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "llama_pearson = [0.4891, 0.4544, 0.4891]\n",
    "llama_spearman = [0.6894, 0.6618, 0.6894]\n",
    "\n",
    "phi3_pearson = [0.5461, 0.592, 0.5528]\n",
    "phi3_spearman = [0.7203, 0.6636, 0.7259]\n",
    "\n",
    "rouge_metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "x = np.arange(len(rouge_metrics))  # [0, 1, 2]\n",
    "width = 0.18\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bars1 = ax.bar(x - 1.5*width, llama_pearson, width, label='LLaMA Pearson', color='#FFB6B9')\n",
    "bars2 = ax.bar(x - 0.5*width, llama_spearman, width, label='LLaMA Spearman', color='#FF6F91')\n",
    "bars3 = ax.bar(x + 0.5*width, phi3_pearson, width, label='Phi-3 Pearson', color='#A3D2CA')\n",
    "bars4 = ax.bar(x + 1.5*width, phi3_spearman, width, label='Phi-3 Spearman', color='#5EAAA8')\n",
    "\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_title('Human - ROUGE Scores Correlation', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rouge_metrics, fontsize=11)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend(loc='upper right', fontsize=10, frameon=True, fancybox=True, framealpha=0.9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grouped_human_rouge_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nsaved all figures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ru3_Obe_-Ds"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cLELUO_bXVgR",
    "yOV5nsBlX9ma",
    "-t4HLEUKk49c",
    "K__LBTm7qGcu",
    "1Q9XnR6KoXVp",
    "Qc3kn-36289T",
    "WxLOIk8mwrGF"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
